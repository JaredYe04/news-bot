import OpenAI from "openai";

const SILICONFLOW_API_URL = "https://api.siliconflow.cn/v1";

export async function generateSummary(newsData, timestamp) {
  const apiKey = process.env.SILICONFLOW_API_KEY;
  
  if (!apiKey) {
    console.warn("âš ï¸  SILICONFLOW_API_KEY not set, skipping summary generation");
    return null;
  }

  // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç¡…åŸºæµåŠ¨çš„ API ç«¯ç‚¹
  const client = new OpenAI({
    apiKey: apiKey,
    baseURL: SILICONFLOW_API_URL,
  });

  // æ„å»ºæ–°é—»å†…å®¹æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨RSSæ‘˜è¦ï¼Œå…¶æ¬¡å…¨æ–‡ï¼‰
  let newsContent = "ä»Šæ—¥ç§‘ç ”ä¸æŠ€æœ¯æ–°é—»å†…å®¹ï¼š\n\n";
  
  for (const block of newsData) {
    if (block.items.length === 0) continue;
    
    newsContent += `ã€${block.category}ã€‘\n`;
    block.items.slice(0, 5).forEach((item, idx) => {
      newsContent += `\n${idx + 1}. ${item.title} (æ¥æº: ${item.source})\n`;
      newsContent += `   é“¾æ¥: ${item.link}\n`;
      
      // ä¼˜å…ˆä½¿ç”¨å…¨æ–‡ï¼Œå…¶æ¬¡RSSæ‘˜è¦ï¼Œæœ€åæ‰ç”¨æ ‡é¢˜
      const content = item.fullContent || item.snippet || "";
      
      if (content && content.trim().length > 50) {
        // é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¶…å‡º token é™åˆ¶
        const trimmedContent = content.length > 800 
          ? content.substring(0, 800).trim() + '...'
          : content.trim();
        
        const contentType = item.contentType === 'fulltext' ? 'å…¨æ–‡' : 'RSSæ‘˜è¦';
        newsContent += `   å†…å®¹ï¼ˆ${contentType}ï¼‰: ${trimmedContent.replace(/\n/g, ' ')}\n`;
      } else {
        newsContent += `   (ä»…æ ‡é¢˜ï¼Œæ— è¯¦ç»†å†…å®¹)\n`;
      }
    });
    newsContent += "\n";
  }

  const prompt = `å½“å‰æ—¶é—´æˆ³ï¼š${timestamp}

ä»¥ä¸‹æ˜¯ä»Šæ—¥ï¼ˆ${new Date(timestamp).toLocaleString('zh-CN', { timeZone: 'Asia/Shanghai' })})æ”¶é›†çš„ç§‘ç ”ä¸æŠ€æœ¯çƒ­ç‚¹æ–°é—»ï¼š

${newsContent}

è¯·ä¸ºä»¥ä¸Šæ–°é—»ç”Ÿæˆä¸€ä»½ç®€æ´çš„ä»Šæ—¥æ€»ç»“ï¼ŒåŒ…æ‹¬ï¼š
1. ä»Šæ—¥æœ€é‡è¦çš„æŠ€æœ¯è¶‹åŠ¿å’Œçƒ­ç‚¹ï¼ˆ3-5ç‚¹ï¼‰
2. å€¼å¾—å…³æ³¨çš„ç ”ç©¶æ–¹å‘æˆ–çªç ´
3. ç®€è¦çš„åˆ†ææˆ–å±•æœ›

è¦æ±‚ï¼šè¯­è¨€ç®€æ´ä¸“ä¸šï¼Œä¸­æ–‡è¾“å‡ºï¼Œ200-300å­—å·¦å³ã€‚`;

  try {
    console.log("ğŸ¤– Generating summary with LLM...");
    
    const response = await client.chat.completions.create({
      model: "Qwen/Qwen2.5-7B-Instruct",
      messages: [
        {
          role: "user",
          content: prompt
        }
      ],
      stream: false,
      max_tokens: 32767,
      thinking_budget: 32767,
      min_p: 0.05,
      temperature: 0.5,
      top_p: 0.7,
      top_k: 50,
      frequency_penalty: 0.5,
      n: 1,
      response_format: {
        type: "text"
      }
    });

    const summary = response.choices[0]?.message?.content?.trim();
    
    if (summary) {
      console.log("âœ“ Summary generated successfully");
      return summary;
    } else {
      console.warn("âš ï¸  Empty summary returned");
      return null;
    }
  } catch (error) {
    console.error("âŒ Failed to generate summary:", error.message);
    return null;
  }
}

